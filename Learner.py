import sklearn
import scipy
import pymongo
import numpy as np
from GEOvector import GEOvector 
import sklearn.naive_bayes
from itertools import izip
class Learner:
    def __init__(self, mongo_loc='clutch', nclasses = 2, stopwords = None, burn=1.0):
        """
        initialize the learner class - pretty much just sets class variables
            mongo_loc - network id of the mongo server
            nclasses - number of classes to learn
            stopwords - name of a file containing words to ignore in training
            burn - initial burn in value (initial weight that an unlabeled example is allowed to contribute to the learning. Increases by .05 every iteration until it reaches 1.0)
        """

        self._conn = pymongo.Connection(mongo_loc)
        self.dictionary = self.getWords() #the mapping word -> vector index
        #temporary data structures used to build sparse matrix
        #kill after sparse matrix created
        self.num_classes = nclasses
        #storage for the labelled data
        self.ldata = []
        self.lrow = []
        self.lcol = []
        self.stopwords = stopwords
        self.burn = 1.0

    def buildLabelCoords( self, geo_ids):
        """
        Convert a GEOvectors into coordinate format to allow conversion to
        scipy.sparse.coo_matrix
            -geo_ids - list of strings containing geo_ids
            -returns (list of data values, list of row values corresponding to data values, list of column values corresponding to data values)
                i.e. ([1.0],[2],[3]) mease that the value in a matrix in the 2nd row in the third column is 1.
        """
        data = []
        row = []
        col = []
        for i, val in enumerate(geo_ids):
            gv = GEOvector(val, self.dictionary, self._conn, stopwords=self.stopwords)
            self.addVecToCoord(gv,i, data, row, col, default=gv.pseudocount)
        return (data, row, col)

    def buildLabeledModel( self, geo_ids, classes, threshold=20):
        """
        Takes the geo_ids and their associated classes and builds the structures necessary to learn from.
            - geo_ids - list of strings containing geo_ids
            - classes - list of classes (integers) that indicate the class of the associated geo_id
                i.e. classes[0] is an integer representing the class that geo_ids[0] is assigned to.
            - threshold - (integer) ignore geo_ids with fewer than this many words
        """
        #clean out bad geo_ids
        n_geo_ids = self.removeWeakGEO(geo_ids, thresh=threshold)
        classes = [ c for c,g in izip(classes,geo_ids) if g in n_geo_ids]
        geo_ids = n_geo_ids

        #make class variables for labeled data
        self.ldata, self.lrow, self.lcol = self.buildLabelCoords( geo_ids )
        self.lclasses = classes
        self.lN = len(geo_ids) #number of rows
        self.M = len(self.dictionary.keys())#number of columns

    def removeWeakGEO(self, geo_ids, thresh=20):
        """
        Checks that the geo_id record has more words than the threshold
            - geo_ids - list of strings containing geo_ids
            - threshold - (integer) ignore geo_ids with fewer than this many words
            returns list of geo_ids that meet the threshold given
        """
        strong = []
        for g in geo_ids: 
            gv =GEOvector(g, self.dictionary, self._conn, stopwords=self.stopwords)         
            if gv.numWords > thresh:
                strong.append(g)
        return strong

    def initUnlabeled(self, geo_ids):
        """
        Takes the unlabelled geo_ids and sets the class variables for learning
            - geo_ids - list of strings containing geo_ids
        """       
        geo_ids = self.removeWeakGEO(geo_ids)
        
        self.udata, self.urow, self.ucol = self.buildLabelCoords( geo_ids )
        self.uN = len(geo_ids) 


    def buildUnlabeledMModel(self, posterior):
        """
        Takes the learned posterior distribution and weights the unlabelled data according to the probability that it was generated by each class
            i.e. if based on the previous run sample 1 has an .80 likelyhood of being from class 1 and a .20 likelihood of being from class 2, a new record is generated that gives .8 of the weight from sample 1 to class one and .2 of the weight from sample 1 to class 2.
            returns - (list of data values, list of row values corresponding to data values, list of column values corresponding to data values, list of classes matching data, number of rows)
        """
        
        data = []
        col = []
        row = []
        classes = []
        uN = self.uN
        lN = self.lN
        #adjust the burn
        #burn in is used to allow the weights given by
        #the samples to grow as the algorithm progresses
        #this is to avoid the unlearned examples overwhelming
        #the learning early
        if self.burn < 1.0:
            self.burn += .05
        else:
            self.burn = 1.0
        for j in range(self.num_classes):
            assert len(self.udata) == len(self.urow)
            assert len(self.ucol) == len(self.urow)
            for d, r, c in izip(self.udata, self.urow, self.ucol):
                pij = posterior[r, j]#P(sample i is from class j)
                data.append(d*pij*self.burn)#weighted data
                row.append(j*uN + r + lN)
                col.append(c)
            for q in range(uN):
                classes.append(j)
        N = uN*self.num_classes
        return (data,row,col,classes, N)

    def EStep(self, learner):
        """
        Returns a matrix containing the probability that that unlabelled data
        was generated by each class
        class x numsamples matrix(numpy)
        """
        #get ulabelled data
        d, r, c = (np.array(self.udata), np.array(self.urow), np.array(self.ucol))
        X = scipy.sparse.coo_matrix( (d, (r, c)), shape= (self.uN, self.M) )
        #get probabilities
        post = learner.predict_proba(X)
        assert post.shape[0] == X.shape[0]
        return learner.predict_proba(X)

        
  
    def MStep(self, posterior):
        """
        Takes the posterior probability that the unlabelled data was generated
        by the current model and generates a new model based on that prob.
            -posterior - matrix of floats (class x num unlabeled samples) (see buildUnlabeledMModel)
            returns - (sklearn.naive_bayes.MultinomialNB object) naive_bayes learner fit to labeled and unlabeled data
        """
        #generate unlabeled data with posterior weighting
        udata, urow, ucol, uclasses, uN = self.buildUnlabeledMModel( posterior )
        #combine labeled and unlabeled data
        comb_data = np.concatenate((self.ldata, udata))
        comb_row = np.concatenate((self.lrow, urow))
        comb_col = np.concatenate((self.lcol, urow))
        comb_N = self.lN + uN
        comb_M = self.M


        X = scipy.sparse.coo_matrix( (comb_data, (comb_row, comb_col)), shape=(comb_N, comb_M)) 
        Y = np.concatenate((self.lclasses, uclasses))

        return self.runBayes(X,Y)

    def runBayes(self, X, Y):
        """
        Runs the learning algorithm on data
            X-sparse matrix with the features (samples x features)
            Y-array mapping each samples to a class (samples x 1)
            returns - (sklearn.naive_bayes.MultinomialNB object) fit to X, Y
        """
        b = sklearn.naive_bayes.MultinomialNB(alpha=1, fit_prior=False)
        b.fit(X, Y)
        return b        
       
    def EM(self, labeled_geo, labeled_classes, unlabeled_geo, max_iter=100):
        """
        Performs the EM algorithm on the data
            labeled_geo - list of strings containing geo_ids of known classes
            labeled_classes - list of integers mapping labeled geo_id -> class
            unlabeled_geo - list of strings containing geo_ids with unknown classes
            max_iter - maximum number of times to run the algorithm, in case it does not converge
            returns trained model
        """
        #build initial class variables for labeled and unlabeled data
        self.buildLabeledModel(labeled_geo, labeled_classes)
        self.initUnlabeled(unlabeled_geo)
        
        #run the learner on labeled data to get the ball rolling
        d, r, c = (np.array(self.ldata), np.array(self.lrow), np.array(self.lcol))
        X =scipy.sparse.coo_matrix( (d, (r, c)), shape= (self.lN, self.M) )
        Y = np.array(self.lclasses)
        bayes_model = self.runBayes(X,Y)

        
        counter = 0#for max_iter
        #vars to test for convergence
        prev_post_sum = 0
        post_sum = 1
        #EM
        while counter < max_iter and prev_post_sum != post_sum:
            print "iteration: "
            print counter
            prev_post_sum = post_sum
            post_sum = 0
            #build posterior dist from model and unlabeled
            post = self.EStep( bayes_model )
            #see if posterior has changed
            for x in post:
                post_sum += (x[0] - x[1])**2
                if x[0] > x[1]:
                    print "kidney"
                else:
                    print "prostate"
            #update model using unlabeled data weighted by the posterior dist 
            bayes_model = self.MStep( post )
            counter += 1
        return bayes_model
        
        
    def getWords(self):
        """
        get all of the words
        """
        collection = self._conn.geo.word2geo
        word_dict = {}
        for i, word in enumerate(collection.distinct("word")):
            word_dict[word] = i
        return word_dict

    def addVecToCoord(self, vec, row_n, data, row, col, default=0):
        """
        build coordinate vector from GEOVector
        """
        wc = 0
        for i, val in enumerate(vec):
            if val != default:
                data.append(val)
                col.append(i)
                row.append(row_n)
            

if __name__ == "__main__":
    l = Learner(stopwords="stop.txt")
    g = ["GDS1023","GDS1282","GDS1344","GDS1411","GDS1438","GDS1499","GDS1548","GDS1700","GDS1713","GDS2018","GDS1390","GDS1423","GDS1439","GDS1697","GDS1699","GDS1736","GDS1746","GDS1973","GDS2034","GDS2171"]
    unlabeled = ["GDS2426","GDS2499","GDS2880","GDS2881","GDS2921","GDS3274","GDS3524","GDS3603","GDS3626","GDS505","GDS507","GDS686","GDS724","GDS892","GDS893","GDS916","GDS961","GDS987","GSE1009","GSE1147","GSE12090","GSE12792","GSE1309","GSE14328","GSE14630","GSE1563","GSE16441","GSE1743","GSE1801","GSE1822","GSE1823","GSE19249","GSE1982","GSE2004","GSE2020","GSE20247","GSE20896","GSE21816","GSE22316","GSE22459","GDS2384","GDS2499","GDS2545","GDS2546","GDS2547","GDS2617","GDS2618","GDS2782","GDS2865","GDS2958","GDS2971","GDS3095","GDS3111","GDS3155","GDS3289","GDS3634","GDS3710","GDS535","GDS536","GDS719","GDS720","GDS721","GDS722","GDS723","GSE11701","GSE15787","GSE16120","GSE17031","GSE17708","GSE18109","GSE1825","GSE18573","GSE18916","GSE18917","GSE18918","GSE19396","GSE19426","GSE19822","GSE20317","GSE20543"]
    c=[0 if i<len(g)/2 else 1 for i in range(len(g))]
    print "starting EM"
    b = l.EM(g, c, unlabeled)
    print "ending EM"
    print b.feature_log_prob_
