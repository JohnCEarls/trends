import sklearn
import scipy
import pymongo
import numpy as np
from GEOvector import GEOvector 
import sklearn.naive_bayes
from itertools import izip
import sklearn.mixture
import sklearn.cluster
import json
import random
class Learner:
    def __init__(self, mongo_loc='clutch', mongo_port=27017, nclasses = 2, stopwords = None, burn=1.0, words_file=None, fit_prior=False, alpha=1.0):
        """
        initialize the learner class - pretty much just sets class variables
            mongo_loc - network id of the mongo server
            nclasses - number of classes to learn
            stopwords - name of a file containing words to ignore in training
            burn - initial burn in value (initial weight that an unlabeled example is allowed to contribute to the learning. Increases by .05 every iteration until it reaches 1.0)
        """

        self._conn = pymongo.Connection(mongo_loc, port=mongo_port)
    
        if words_file is None:
            self.dictionary = self.getWords() #the mapping word -> vector index
        else:
            f = open(words_file,'r')
            self.dictionary = json.loads(f.read())
            f.close()
        self.fit_prior = fit_prior
        self.M = len(self.dictionary.keys())
        #temporary data structures used to build sparse matrix
        #number of columns
        #kill after sparse matrix created
        self.num_classes = nclasses
        #storage for the labelled data
        self.ldata = []
        self.lrow = []
        self.lcol = []
        self.stopwords = stopwords
        self.burn =burn
        self.alpha = alpha
        self.bayes_model = None

    def buildLabelCoords( self, geo_ids):
        """
        Convert a GEOvectors into coordinate format to allow conversion to
        scipy.sparse.coo_matrix
            -geo_ids - list of strings containing geo_ids
            -returns (list of data values, list of row values corresponding to data values, list of column values corresponding to data values)
                i.e. ([1.0],[2],[3]) mease that the value in a matrix in the 2nd row in the third column is 1.
        """
        data = []
        row = []
        col = []
        for i, val in enumerate(geo_ids):
            gv = GEOvector(val, self.dictionary, self._conn, stopwords=self.stopwords)
            self.addVecToCoord(gv,i, data, row, col)
        return (data, row, col)

    def buildLabeledModel( self, geo_ids, classes, threshold=20):
        """
        Takes the geo_ids and their associated classes and builds the structures necessary to learn from.
            - geo_ids - list of strings containing geo_ids
            - classes - list of classes (integers) that indicate the class of the associated geo_id
                i.e. classes[0] is an integer representing the class that geo_ids[0] is assigned to.
            - threshold - (integer) ignore geo_ids with fewer than this many words
        """
        #clean out bad geo_ids
        n_geo_ids = self.removeWeakGEO(geo_ids, thresh=threshold)
        classes = [ c for c,g in izip(classes,geo_ids) if g in n_geo_ids]
        geo_ids = n_geo_ids

        #make class variables for labeled data
        self.ldata, self.lrow, self.lcol = self.buildLabelCoords( geo_ids )
        self.lclasses = classes
        self.lN = len(geo_ids) #number of rows

    def removeWeakGEO(self, geo_ids, thresh=20):
        """
        Checks that the geo_id record has more words than the threshold
            - geo_ids - list of strings containing geo_ids
            - threshold - (integer) ignore geo_ids with fewer than this many words
            returns list of geo_ids that meet the threshold given
        """
        strong = []
        for g in geo_ids: 
            gv =GEOvector(g, self.dictionary, self._conn, stopwords=self.stopwords)         
            if gv.numWords > thresh:
                strong.append(g)
        return strong

    def initUnlabeled(self, geo_ids):
        """
        Takes the unlabelled geo_ids and sets the class variables for learning
            - geo_ids - list of strings containing geo_ids
        """       
        geo_ids = self.removeWeakGEO(geo_ids)
        
        self.udata, self.urow, self.ucol = self.buildLabelCoords( geo_ids )
        self.uN = len(geo_ids) 


    def buildUnlabeledMModel(self, posterior):
        """
        Takes the learned posterior distribution and weights the unlabelled data according to the probability that it was generated by each class
            i.e. if based on the previous run sample 1 has an .80 likelyhood of being from class 1 and a .20 likelihood of being from class 2, a new record is generated that gives .8 of the weight from sample 1 to class one and .2 of the weight from sample 1 to class 2.
            returns - (list of data values, list of row values corresponding to data values, list of column values corresponding to data values, list of classes matching data, number of rows)
        """
        
        data = []
        col = []
        row = []
        classes = []
        uN = self.uN
        lN = self.lN
        #adjust the burn
        #burn in is used to allow the weights given by
        #the samples to grow as the algorithm progresses
        #this is to avoid the unlearned examples overwhelming
        #the learning early
        if self.burn < 1.0:
            self.burn += .05
        else:
            self.burn = 1.0
        skip = 0
        post_sort = np.argsort(posterior, axis=0)
        prev_r = -1
        min_order = uN -self.k #the kth largest elements sorted value
        row_counter = 0
        added = {}
        c_count = [0,0,0]
        for r in range(uN):
            for j in range(self.num_classes):
                if posterior[r,j] > .5:
                    classes.append(j)
                    added[r] = row_counter
                    row_counter += 1
                    c_count[j] += 1
        #I pity the fool with no matches
        for i, cc in enumerate(c_count):
            if cc == 0:#no samples
                pity = random.sample([x for x in range(uN)],max(sum(c_count)/3, 3 ))
                for p in pity:
                    classes.append(i)
                    added[p] = row_counter
                    row_counter += 1
                    c_count[j] += 1

                

            
                
         
        for d,r,c in izip(self.udata, self.urow, self.ucol):
            if r in added:
                pij = posterior[r, classes[added[r]]]#P(sample i is from class j)
                #pij = 1.0
                row.append(added[r]) #(j*uN + r + lN - skip)
                data.append(d*pij*self.burn)#weighted data
                #data.append(d)#weighted data
                col.append(c)
           
       

        """
        for j in range(self.num_classes):
            skip = 0
            assert len(self.udata) == len(self.urow)
            assert len(self.ucol) == len(self.urow)
            for d, r, c in izip(self.udata, self.urow, self.ucol):
                if  posterior[r, j] > .5:#post_sort[r,j] >= min_order:
                    pij = posterior[r, j]#P(sample i is from class j)
                    
                    row.append(j*uN + r + lN - skip)
                    data.append(d*pij*self.burn)#weighted data
                    #data.append(d)#weighted data
                    col.append(c)
                    if p_r != r:
                        #print "class " + str(j)
                        #print "posterior :" +str(pij)
                        classes.append(j)
                elif p_r != r:#only add to skip if new row (e.g. new geo_id)
                    skip +=1
                p_r = r
                if skip == uN:
                    print "Class " + str(j) + " is empty"
            
            print uN-skip 
            for q in range(uN-skip):
                classes.append(j)
            """
        return (data,row,col,classes, len(classes))

    def EStep(self, learner):
        """
        Returns a matrix containing the probability that that unlabelled data
        was generated by each class
        class x numsamples matrix(numpy)
        """
        #get ulabelled data
        d, r, c = (np.array(self.udata), np.array(self.urow), np.array(self.ucol))
        X = scipy.sparse.coo_matrix( (d, (r, c)), shape= (self.uN, self.M) )
        #get probabilities
        post = learner.predict_proba(X)
        assert post.shape[0] == X.shape[0]
        return learner.predict_proba(X)

        
  
    def MStep(self, posterior):
        """
        Takes the posterior probability that the unlabelled data was generated
        by the current model and generates a new model based on that prob.
            -posterior - matrix of floats (class x num unlabeled samples) (see buildUnlabeledMModel)
            returns - (sklearn.naive_bayes.MultinomialNB object) naive_bayes learner fit to labeled and unlabeled data
        """
        #generate unlabeled data with posterior weighting
        udata, urow, ucol, uclasses, uN = self.buildUnlabeledMModel( posterior )
        #combine labeled and unlabeled data
        comb_data = np.concatenate((self.ldata, udata))
        comb_row = np.concatenate((self.lrow, urow))
        comb_col = np.concatenate((self.lcol, urow))
        comb_N = self.lN + uN
        comb_M = self.M


        X = scipy.sparse.coo_matrix( (comb_data, (comb_row, comb_col)), shape=(comb_N, comb_M)) 
        Y = np.concatenate((self.lclasses, uclasses))

        return self.runBayes(X,Y)

    def runBayes(self, X, Y):
        """
        Runs the learning algorithm on data
            X-sparse matrix with the features (samples x features)
            Y-array mapping each samples to a class (samples x 1)
            returns - (sklearn.naive_bayes.MultinomialNB object) fit to X, Y
        """
        b = sklearn.naive_bayes.MultinomialNB(alpha=self.alpha, fit_prior=self.fit_prior)
        b.fit(X, Y)
        return b        
       
    def EM(self, labeled_geo, labeled_classes, unlabeled_geo, max_iter=100):
        """
        Performs the EM algorithm on the data
            labeled_geo - list of strings containing geo_ids of known classes
            labeled_classes - list of integers mapping labeled geo_id -> class
            unlabeled_geo - list of strings containing geo_ids with unknown classes
            max_iter - maximum number of times to run the algorithm, in case it does not converge
            returns trained model
        """
        #build initial class variables for labeled and unlabeled data
        self.buildLabeledModel(labeled_geo, labeled_classes)
        self.initUnlabeled(unlabeled_geo)
        
        #run the learner on labeled data to get the ball rolling
        d, r, c = (np.array(self.ldata), np.array(self.lrow), np.array(self.lcol))
        X =scipy.sparse.coo_matrix( (d, (r, c)), shape= (self.lN, self.M) )
        Y = np.array(self.lclasses)
        bayes_model = self.runBayes(X,Y)
        if False:
            self.bayes_model = bayes_model
            return bayes_model
        
        counter = 0#for max_iter
        #vars to test for convergence
        prev_post_sum = 0
        post_sum = 1
        #EM
        self.k = self.lN/self.num_classes
        while counter < max_iter and prev_post_sum != post_sum:
            if self.k > self.uN and self.burn >= 0.5:#only start looking for convergence after using all samples
               prev_post_sum = post_sum
            post_sum = 0
            #build posterior dist from model and unlabeled
            post = self.EStep( bayes_model )
            #see if posterior has changed
            for x in post:#need better convergence
                post_sum += sum([x_i**2 for x_i in x])
            #update model using unlabeled data weighted by the posterior dist 
            bayes_model = self.MStep( post )
            counter += 1
            self.k += self.k #double number of samples to add each time
        self.bayes_model = bayes_model
        print "iterations: ",
        print counter -1
        return bayes_model
        
     
    def getWords(self):
        """
        get all of the words
        """
        collection = self._conn.geo.word2geo
        word_dict = {}
        for i, word in enumerate(collection.distinct("word")):
            word_dict[word] = i
        return word_dict

    def addVecToCoord(self, vec, row_n, data, row, col):
        """
        build coordinate vector from GEOVector
        vec - GEOvector object
        row_n - integer for the row this vector will occupy in matrix
        data, row, col - data, row, col lists to be appended to

        """
        wc = 0
        d,v =  vec.getSparse()
        for gv_data, gv_col in izip(d,v):
            data.append(gv_data)
            col.append(gv_col)
            row.append(row_n)
            
    def predict(self, geo_id):
        geo_vec = GEOvector(geo_id, self.dictionary, self._conn, stopwords=self.stopwords)         
        data, row, col = ([],[], [])
        self.addVecToCoord(geo_vec,0, data, row, col) 
        
        return self.bayes_model.predict(scipy.sparse.coo_matrix((data, (row, col)), shape=(1, self.M)))[0]

    def cluster(self, geo_ids, max_num_clusters, threshold = 20):
        print len(geo_ids)
        geo_ids = self.removeWeakGEO(geo_ids, thresh=threshold)

        data, row, col = self.buildLabelCoords( geo_ids )        
        print self.M
        print len(geo_ids)
        mymat = scipy.sparse.coo_matrix((data, (row, col)), shape=(len(geo_ids), self.M))
        #print mymat
        print mymat.shape
        self.gmm = sklearn.cluster.KMeans(k=2)  #sklearn.mixture.GMM(n_components=max_num_clusters)
        self.gmm.fit( mymat )
        print self.gmm.n_components
        print self.gmm.means

        
    def cluster_identity( self, geo_id ):
        data, row, col = self.buildLabelCoords( [geo_id] )
        mymat = scipy.sparse.coo_matrix((data, (row, col)), shape=(1, self.M))
        print mymat
        return self.gmm.predict(mymat)
    
if __name__ == "__main__":
    l = Learner(mongo_loc='clutch',stopwords="stop.txt")
    g = ["GDS1023","GDS1282","GDS1344","GDS1411","GDS1438","GDS1499","GDS1548","GDS1700","GDS1713","GDS2018","GDS1390","GDS1423","GDS1439","GDS1697","GDS1699","GDS1736","GDS1746","GDS1973","GDS2034","GDS2171"]
    unlabeled = ["GDS2426","GDS2499","GDS2880","GDS2881","GDS2921","GDS3274","GDS3524","GDS3603","GDS3626","GDS505","GDS507","GDS686","GDS724","GDS892","GDS893","GDS916","GDS961","GDS987","GSE1009","GSE1147","GSE12090","GSE12792","GSE1309","GSE14328","GSE14630","GSE1563","GSE16441","GSE1743","GSE1801","GSE1822","GSE1823","GSE19249","GSE1982","GSE2004","GSE2020","GSE20247","GSE20896","GSE21816","GSE22316","GSE22459","GDS2384","GDS2499","GDS2545","GDS2546","GDS2547","GDS2617","GDS2618","GDS2782","GDS2865","GDS2958","GDS2971","GDS3095","GDS3111","GDS3155","GDS3289","GDS3634","GDS3710","GDS535","GDS536","GDS719","GDS720","GDS721","GDS722","GDS723","GSE11701","GSE15787","GSE16120","GSE17031","GSE17708","GSE18109","GSE1825","GSE18573","GSE18916","GSE18917","GSE18918","GSE19396","GSE19426","GSE19822","GSE20317","GSE20543"]
    c=[0 if i<len(g)/2 else 1 for i in range(len(g))]
    print "starting cluster"
    #l.cluster(unlabeled, 2)
    #for u in unlabeled:
    #    print l.cluster_identity(u)
    print "starting EM"
    
    g=  ["GSM258553", "GSM258555", "GSM258556", "GSM258557", "GSM258562", "GSM258563", "GSM258565", "GSM258566", "GSM258570", "GSM258578", "GSM258580", "GSM258583", "GSM258585", "GSM258590","GSM15749", "GSM15750", "GSM15751", "GSM15752", "GSM15753", "GSM15754", "GSM15755", "GSM15756", "GSM15757", "GSM15758"]    
    c=[0 if i<len(g)/2 else 1 for i in range(len(g))]
    unlabeled = ["GSM101105", "GSM101106", "GSM101107", "GSM101108", "GSM101109", "GSM101110", "GSM101111", "GSM101112", "GSM101113", "GSM101114", "GSM101115", "GSM101116", "GSM272770", "GSM272771", "GSM272772", "GSM210005", "GSM210009", "GSM210014", "GSM210015", "GSM210087", "GSM210192", "GSM210196", "GSM210979", "GSM211008", "GSM212067", "GSM212068", "GSM212070", "GSM212787", "GSM212789", "GSM212790", "GSM212811", "GSM212853", "GSM213035", "GSM213036", "GSM114089", "GSM114090", "GSM190151", "GSM190153", "GSM252879", "GSM252882", "GSM252884", "GSM252885", "GSM254149", "GSM254150", "GSM254151", "GSM254152", "GSM254157", "GSM254158", "GSM254159", "GSM254160", "GSM254161", "GSM469508", "GSM469513", "GSM469515", "GSM469516", "GSM469517", "GSM469519", "GSM469521", "GSM28358", "GSM28360", "GSM28362", "GSM28370", "GSM28372", "GSM28374", "GSM28376", "GSM28378", "GSM28380", "GSM28382", "GSM28384", "GSM28386", "GSM309986", "GSM309987", "GSM309988", "GSM309989", "GSM475657", "GSM475658", "GSM475659", "GSM475660", "GSM475663", "GSM475665", "GSM475666", "GSM475667", "GSM475669", "GSM475671", "GSM475673", "GSM475675", "GSM475678", "GSM475680", "GSM475682", "GSM475684", "GSM475686", "GSM475688", "GSM475690", "GSM475693", "GSM475695", "GSM475697", "GSM475699", "GSM475702", "GSM475704", "GSM475705", "GSM475707", "GSM475711", "GSM475714", "GSM475716", "GSM475718", "GSM475721", "GSM475723", "GSM475725", "GSM475726", "GSM475729", "GSM475732", "GSM475734", "GSM475736", "GSM475738", "GSM475740", "GSM475742", "GSM475743", "GSM475745", "GSM370937", "GSM370940", "GSM370942", "GSM370943", "GSM370945", "GSM370946", "GSM370947", "GSM370948", "GSM370949", "GSM370952", "GSM370953", "GSM370954", "GSM370957", "GSM370959", "GSM370960", "GSM370961", "GSM370963", "GSM370964", "GSM370965", "GSM370966", "GSM370968", "GSM370969", "GSM370970", "GSM370971", "GSM370973", "GSM370974", "GSM370975", "GSM370978", "GSM370981", "GSM370982", "GSM370983", "GSM370988", "GSM370989", "GSM370990", "GSM370991", "GSM370993", "GSM370994", "GSM370995", "GSM370996", "GSM370998", "GSM475664", "GSM475668", "GSM475674", "GSM475694", "GSM475713", "GSM475719", "GSM475728", "GSM475730", "GSM475731", "GSM475733", "GSM475735", "GSM475744", "GSM475748", "GSM475751", "GSM475753", "GSM475756", "GSM475758", "GSM475759", "GSM475760", "GSM475762", "GSM475763", "GSM475778", "GSM475782", "GSM475789", "GSM475801", "GSM475804", "GSM475806"]
    print "starting cluster"
    b = l.EM(g, c, unlabeled)
    print "ending EM"
    for u in unlabeled:
        print u + ":"
        print l.predict(u)
    """
    kidney_p  = [(i, p) for i, p in enumerate(b.feature_log_prob_[0])]
    prostate_p  = [(i, p) for i, p in enumerate(b.feature_log_prob_[1])]
    kidney_p.sort(key=lambda x: x[1], reverse=True)
    prostate_p.sort(key=lambda x: x[1],reverse=True)
    mymap = ['m' for x in l.dictionary.keys()]
    for k, v in l.dictionary.iteritems():
        mymap[v] = k
    count = 0
    for kid, pro in izip(kidney_p, prostate_p):
        print mymap[kid[0]] + str(kid[1]) + " --- " + mymap[pro[0]] + str(pro[1])
        if count > 100:
            break
        count += 1
    """

